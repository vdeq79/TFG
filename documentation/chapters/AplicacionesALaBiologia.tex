\chapter{Aplicaciones a la biología}

En este capítulo, introduciremos algunos problemas de biología computacional y de bioinformática que pueden ser resueltos mediante el uso de HMM. Empezaremos presentado algunos conceptos básicos de biología relacionados con nuestro estudio, discutiremos sobre la naturaleza y la importancia que tienen los problemas y cómo podemos adaptar el modelo y los algoritmos vistos en el capítulo anterior para resolverlos.

Para este capítulos, las fuentes principales son \cite{Durbin}, \cite{Yoon} y \cite[Capítulo 8]{Vidyasagar}.

\section{Nociones básicas de biología}
Para nuestro estudio necesitamos introducir algunas nociones básicas de biología, en concreto, presentaremos conceptos relacionados con el ADN y las proteínas. Cabe destacar que por el carácter que tiene este trabajo, no entraremos en detalle sobre estos conceptos y sólo vamos a exponer la información relevante para poder introducir los problemas. Por lo tanto, el lector puede encontrar en determinadas ocasiones, un falta de rigor en el sentido de biología. Por último, este apartado se base esencialmente en \cite[Capítulo 8]{Vidyasagar} y \cite[Apéndice A]{Warren}.

El material genético para la mayoría de los seres vivos es el ácido desoxirribonucleico, conocido generalmente como ADN. Consiste en un polímero (conjunto) de nucleótidos, en los que cada nucleótido está compuesto por un glúcido (la desoxirribosa), un grupo fosfato y una base nitrogenada de uno de los siguientes cuatro tipos: adenina, guanina, citosina y timina. En general, se denota a cada nucleótido por la letra inicial de la base que contiene, es decir, por $A,\, G,\, C$ y $T$ respectivamente. 

Nucleótidos adyacentes en una de las cadenas del ADN se conectan mediante un vínculo químico entre el glúcido de uno y el grupo fosfato del siguiente. La estructura clásica de doble hélice del ADN se forma cuando se conectan las dos cadenas de nucleótidos mediante puentes de hidrógeno. Estas conexiones sólo se forman para pares de nucleótidos concretos (conocidos como par de bases): la adenina con la timina ($A\leftrightarrow T$) y la guanina con la citosina ($G\leftrightarrow C$). Por lo tanto, las dos cadenas de ADN son complementarias pues si una cadena contiene una $A$, entonces estará conectada con una $T$ en la cadena contraria. Análogamente, si una contiene una $C$, la otra contendrá una $G$. 

\begin{figure}[H]
    \centering
    \includegraphics[width=6cm]{img/Estructura_del_ADN.jpg}
    \caption{Estructura del ADN}
\end{figure}

La secuencia en la que constituyen las diferentes bases en una de las cadenas del ADN representa la información genética codificada en dicha cadena. Por la complementariedad de las cadenas, a partir de una de ellas se puede también determinar la información de la otra. Además, cada cadena tiene definida una dirección espacial contraria a la otra, de modo que sólo podemos interpretar una cadena en la dirección correspondiente.

A nivel celular, el ADN se organiza en cromosomas, cada uno de los cuales contiene un ADN que puede tener cientos de millones de par de bases. La mayoría de las células humanas contienen 23 pares de cromosomas, uno heredado del padre y el otro de la madre. Los dos cromosomas de un par son prácticamente idénticos, con la excepción del cromosoma sexual, para el cual existen dos tipos, X e Y. Casi todas las células del cuerpo humano contienen copias idénticas del conjunto completo de 23 pares de cromosomas. El conjunto total de ADN de un organismo se conoce como su genoma, cabe destacar que el genoma humano contiene a más de tres mil millones de pares de bases.

Un cromosoma humano está compuesto principalmente por ADN no codificante, cuya función apenas se está empezando a entender. Interpuesto en el ADN se encuentran genes que codifican proteínas. Estos genes representan aproximadamente el $2\%$ del genoma humano, sin embargo, son el foco de atención de los genetistas. Los genes a menudo están organizados en exones, que son secuencias que eventualmente serán utilizadas por la célula, alternado con intrones, que serán descartados en el proceso de codificación. La información en estos genes se codificará en ARN (ácido ribonucleico), y en muchos casos, finalmente en proteínas.

En el proceso de codificación de un gen a un ARN, se utiliza la secuencia de ADN del gen (eliminano los intrones) como plantilla. Al igual que el ADN, el ARN está también compuesto por una serie de nucleótidos, pero con ciertas diferencias: el ARN está formado en general por una única cadena y sustituye la base nitrogenada uracilo ($U$) por la timina ($T$). Un caso particular de ARN, el ARNm (ARN mensajero), será finalmente transformado en proteína.

Una proteína está compuesta por una secuencia de aminoácidos, existen una gran variedad de aminoácidos pero sólo $20$ aparecen en las proteínas. Cada uno de estos aminoácidos está representado por una o más secuencias de tres nucleótidos de ARN conocidas como codones. La combinación de cuatro posibles nucleótidos en grupos de tres resulta en $4^3=64$ codones, lo que significa que la mayoría de los aminoácidos están codificados por más de un codón. La función de una proteína depende finalmente tanto de su secuencia de aminoácidos como de la estructura tridimensional que ha adquirido de su transformación a partir de un ARNm. 

\section{Software utilizado}
Antes de presentar las aplicaciones de HMM en la biología, presentamos los recursos software que vamos a utilizar para ilustrar algunos ejemplos. Como recurso principal se va a utilizar dos librerías de Python:
\begin{itemize}
    \item \textbf{NumPy}: es una de las librerías más utilizadas de Python, proporciona la capacidad de tratar elementos matemáticos de forma sencilla y eficiente. En este caso se ha utilizado la versión más reciente hasta el momento, la 1.24.3. Se puede consultar la documentación en \cite{numpy}.
    \item \textbf{hmmlearn}: es una librería de códigos abiertos para Python que implementa modelos de Markov ocultos. Utiliza códigos escrito en C++ para los algoritmos, de forma que son más eficientes que si son implementados directamente en Python. También implementa modelos que no hemos visto en este trabajo. Se ha utilizado para este trabajo la versión 0.3.0. Se puede consultar la documentación en \cite{hmmlearn} y el código en \cite{hmmlearnGithub}
\end{itemize}
A partir de estas herramientas se implementarán archivos de Jupyter Notebook que nos servirán para ilustrar mediante ejemplos algunas de las aplicaciones de HMM en la biología. 

\section{Islas CpG}
En biología computacional, la predicción de genes es un problema en el que se busca identificar regiones codificadoras o genes en un ADN. Puesto que estas regiones poseen ciertas periodicidades y propiedades estadísticas, los HMM son utilizados para este problema. Considerando las estructuras de los genes como estados ocultos y los pares de bases del ADN como observaciones, la predicción de genes se puede solucionar aplicando el algoritmo de Viterbi. Este razonamiento es aplicable también en otros problemas de análisis biológico como la búsqueda de regiones funcionales, extracción de patrones, búsqueda de motivos de secuencia e identificación de islas CpG. Este último, será el objetivo de nuestro estudio en este apartado \cite{bioStudies}.

Las islas CpG son regiones de ADN con una gran concentración de dinucletidos CpG, que son citosinas ($C$) seguido de guaninas ($G$). Se definen formalmente como regiones de al menos 200 pares de bases con una proporción de $C$ o $G$ superior al de $50\%$ y con un ratio CpG de observado/esperado superior al de $60\%$. Están íntimamente relacionadas con el inicio de un gen en numerosos genomas de mamíferos, por tanto la presencia de una isla CpG es importante en la predicción de genes. Podemos utilizar HMM para determinar si un fragmento corto de ADN proviene de una isla CpG o para encontrar todas las islas CpG en un segmento largo.

Existen diversos modelos que sirven para este problema, vamos a presentar un modelo casi trivial en el que consta de $2$ estados (con los símbolos + y - presentando la pertenencia a una isla CpG o no):

\begin{figure}[H]
\centering
\begin{tikzpicture}[-latex ,auto , node distance =2.2 cm ,semithick, main/.style = {draw, circle, minimum size=1cm}] 
    \node[rectangle, draw, align=center] (1) {Inicio};
    \node[main] (2) [left of=1, below of=1] {+};
    \node[main] (3) [right of=1, below of=1] {-};
    \node[rectangle, draw, align=center] (4) [below of=2]{$C$ o $G$: 0.75\\$A$ o $T$: 0.25};
    \node[rectangle, draw, align=center] (5) [below of=3]{$C$ o $G$: 0.4\\$A$ o $T$: 0.6};

    \draw (1) -- node[above=0.2cm] {0.5} (2);
    \draw (1) -- node[above=0.2cm] {0.5} (3);
    \path (2) edge [loop above] node {0.9} (2);
    \draw (2.10) -- node[midway] {0.1} (3.170);
    \path (3) edge [loop above] node {0.9} (3);
    \draw (3. 210) -- node[midway] {0.1} (2.330);
    
    \draw (2) -- node[] {} (4);
    \draw (3) -- node[] {} (5);
\end{tikzpicture}
\caption{HMM sencillo para identificar islas CpG}
\end{figure}

Usando este modelo podemos utilizar el algoritmo de Viterbi para identificar islas CpG. Como ejemplo, consideramos la siguiente secuencia:

\seqsplit{TCTCGCTGCCGCCAACCCTCGGCGCCGTCGGGTTCGCCGCGGCTCTGATAAGTCCCGTTTATGGTACCCGGGCCGATCTCTGGTGGGAATCGGAGACCTGTGTACCCTGACGCATCCGTTTGTGTTCCCTACACGGCCGACGCAGACCGGGCGCGCGGCGCCACCCAACGAAGCCCGGGTATGGCACGTGCCCCAGGCGGTGCCCTACCCGTATTTCGGGACAAGTTCCCGGATCGGGTGAAAGTTAACGGAAGGATGCCAAGCAATAGCGGCCACAGGACCCGCCTGGCGACGCATGGACTGGATCCGGAGGTCTGGCCAACAGTTGATTTCATGGGTTACAGCCCCGGTGTAGATCCCCTCATGGTCTCCCGAACCGATTAGTTTGAAAACTGTATCTCCTGGCCGCCTAACAGGTATAAAGAGCCGGCTCACACTGGGGTGAGGGGGCGCGTGGCCCCCTT}


Para determinar si proviene de una isla CpG aplicamos el algoritmo de Viterbi utilizando funcionalidades de \textbf{hmmlearn}. Obtenemos el siguiente secuencia de estados:

\textbf{\seqsplit{+++++++++++++++++++++++++++++++++++++++++++-----------------------+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++---------------------------------------------------------++++++++++++++++++++++++++++++++++++++++++++++++++++-----------------------++++++++++++++++++++++++++++++++++++---------------------++++++++++---------------+++++++++++++++++++++++++++++++++++++++}}

Podemos usar este resultado para distinguir los estados correspondientes en la secuencia de observaciones:

\textcolor{cyan}{\seqsplit{TCTCGCTGCCGCCAACCCTCGGCGCCGTCGGGTTCGCCGCGGC}}\textcolor{red}{\seqsplit{TCTGATAAGTCCCGTTTATGGTA}}\textcolor{cyan}{\seqsplit{CCCGGGCCGATCTCTGGTGGGAATCGGAGACCTGTGTACCCTGACGCATCCGTTTGTGTTCCCTACACGGCCGACGCAGACCGGGCGCGCGGCGCCACCCAACGAAGCCCGGGTATGGCACGTGCCCCAGGCGGTGCCCTACCCG}}\textcolor{red}{\seqsplit{TATTTCGGGACAAGTTCCCGGATCGGGTGAAAGTTAACGGAAGGATGCCAAGCAATA}}\textcolor{cyan}{\seqsplit{GCGGCCACAGGACCCGCCTGGCGACGCATGGACTGGATCCGGAGGTCTGGCC}}\textcolor{red}{\seqsplit{AACAGTTGATTTCATGGGTTACA}}\textcolor{cyan}{\seqsplit{GCCCCGGTGTAGATCCCCTCATGGTCTCCCGAACCG}}\textcolor{red}{\seqsplit{ATTAGTTTGAAAACTGTATCT}}\textcolor{cyan}{\seqsplit{CCTGGCCGCC}}\textcolor{red}{\seqsplit{TAACAGGTATAAAGA}}\textcolor{cyan}{\seqsplit{GCCGGCTCACACTGGGGTGAGGGGGCGCGTGGCCCCCTT}}

Donde el color azul representa que las letras provienen de un estado + y el color rojo representa que provienen de un estado - como resultado de aplicar el algoritmo de Viterbi. Si analizamos las secuencias en rojo, podemos ver que contienen una proporción de $T$ o $A$ superior al de $50\%$. Pero en definitiva, tenemos una mayor proporción de estados + en la secuencia de estados resultante, lo que sugiere que la secuencia de observaciones puede provenir de una isla CpG.

\section{Alineamiento de pares de secuencias}
En análisis de secuencias, muchas veces es importante comparar dos secuencias para determinar si están funcionalmente relacionadas. Por ejemplo, genes con funciones similares en diferentes organismos suelen tener secuencias de ADN muy similares. Si un gen nuevo es suficientemente similar a otro gen de otro organismo cuya función es conocida, entonces es razonable esperar que el nuevo gen ejerza la misma función. Lo mismo ocurre con las proteínas, si se descubre una nueva proteína del que se desconoce su estructura tridimensional pero con una similitud sustancial entre su secuencia de aminoácidos y la secuencia de una proteína del que sí se conoce la estructura tridimensional, entonces es razonable esperar que la estructura de la nueva proteína sea de alguna forma similar al de la proteína conocida.

Para estudiar este problema, comenzamos estableciendo algunas notaciones. Vamos a considerar un par de secuencias, $x$ e $y$ de longitudes $n$ y $m$ respectivamente. Sea $x_i$ el $i$-ésimo símbolo de $x$ e $y_j$ el $j$-ésimo símbolo de $y$, estos símbolos pertenecen a un cierto alfabeto $\mathcal{A}$ que en el caso del ADN serán las $4$ bases $\{A,C,G,T\}$ y en el caso de las proteínas serán los $20$ aminoácidos.

Si sólo consideramos las secuencias originales el problema sería trivial: sólo existe un único alineamiento en el caso de que $n=m$ y en otro caso, el problema se reduciría en encontrar la mejor posición para incluir una secuencia en otra. El problema real se tiene en cuenta posibles inserciones de huecos (\textit{gaps}) en cualquiera de las dos secuencias (sin posibilidad de insertar hueco en ambas secuencias al mismo tiempo) para obtener alineamientos entre símbolos iguales. Consideramos el siguiente ejemplo:

\begin{exampleth}
    Tomamos dos secuencias de ADN:
    \begin{center}
        $x=CACGAAT$, $y=AGTTCAA$
    \end{center}
    Podemos considerar un alineamiento entre estas dos secuencias como sigue:
    \begin{center}
        $C\; A - - - C\; G\; A\; A\; T$ \\
        $-\, A\; G\; T\; T\; C - A\; A\, -$
    \end{center}
    donde cada $-$ representa un hueco.
\end{exampleth}

Como podemos apreciar en el ejemplo, las inserciones producen nuevos alineamientos a tener en cuenta. Para valorar los alineamientos, se define una matriz de puntuaciones que asigna para cada coincidencia de símbolos un valor. Esta matriz se conoce usualmente como la matriz de sustitución, un ejemplo de matriz de sustitución para secuencias de ADN podría ser la siguiente:
\[
S=\begin{blockarray}{ccccc}
 & A & C & G & T \\
\begin{block}{c(cccc)}
  A & 10 & -3 & -2 & 1 \\
  C & -2 & 8 & 1 & -2 \\
  G & -3 & 1 & 9 & -3 \\
  T & 0 & -3 & -2 & 6\\
\end{block}
\end{blockarray}
 \]
En esta matriz, cada elemento representa la puntuación que obtendría una coincidencia entre el símbolo de la fila y el símbolo de la columna. Cada uno de estos valores, se puede calcular de la siguiente manera:
\[s(a,b)=\log\left(\dfrac{p_{ab}}{q_a\cdot q_b}\right)\]
siendo $a$ y $b$ elementos del alfabeto $\mathcal{A}$, $p_{ab}$ la probabilidad de que se produzca un emparejamiento entre $a$ y $b$ y $q_a$ la frecuencia relativa esperada de que se produzca un símbolo $a$ en una secuencia. 

Estos valores son relevantes pues afectan a la significación final del análisis: si tenemos un sistema de puntuación ajustado a la realidad, podremos afirmar con mayor seguridad las similitudes entre dos secuencias. Por esta razón, existen métodos para derivar estos valores a partir de datos conocidos y matrices de sustitución utilizadas ampliamente como las matrices PAM o BLOSUM.

Además de las coincidencias entre los símbolos del alfabeto, se tiene en cuenta también las coincidencias entre un símbolo y un hueco. Estas coincidencias tienen una puntuación negativa, lo que se conoce generalmente como penalizaciones por hueco (\textit{gap penalties}). El coste asociado a una consecución de huecos de longitud $g$ puede venir dado por una de las dos siguientes funciones lineares:
\[\gamma_1(g)=-gd \]
\[\gamma_2(g)=-d-(g-1)e\]
donde $d$ se considera la penalización por iniciar la secuencia de huecos y $e$ se considera la penalización por extender la secuencia, usualmente con un valor menor que $d$. Mientras que en $\gamma_1$ se trata todos los huecos por igual, en $\gamma_2$ se penaliza menos las secuencias de huecos con mayores longitudes. Esto es deseable cuando se prevé que las inserciones de varios huecos sean tan frecuentes como inserciones de un único hueco. En la práctica, los valores $d$ y $e$ se escogen empíricamente una vez elegido los valores de la matriz de sustitución.

Con estos elementos, podemos determinar el mejor alineamiento con huecos entre dos secuencias entendiéndolo como aquel con mayor puntuación dada la matriz de sustitución. Si consideramos las dos secuencias enteras, estaremos hablando del alineamiento global de pares de secuencias. Si lo que buscamos es el mejor alineamiento entre subsecuencias de $x$ e $y$, entonces estaremos hablando del alineamiento local.

Para nuestro estudio, vamos a centrarnos en el problema de alineamiento global. Este problema se puede resolver empleando el algoritmo de Needleman-Wunsch, un algoritmo de programación dinámica que garantiza encontrar el alineamiento óptimo entre dos secuencias con posibles huecos. Pero también podemos utilizar un tipo específico de HMM para resolver este problema, los \textit{Pair HMMs}.

\subsection{Pair HMM}
A diferencia de los HMMs que habíamos presentado hasta ahora, los \textit{pair HMMs} generan dos salidas en cada estado en lugar de uno, es decir, tendremos dos procesos $\{\mathcal{Y}_k\}$ y $\{\mathcal{Z}_k\}$ de salida. Por esta razón, podemos utilizar este modelo para el problema de alineamiento de pares. Existen distintas variaciones de este modelo, vamos a presentar el modelo ilustrado en \cite{Durbin}. 

%El espacio de salidas del modelo está compuesto por los posibles alineamientos que pueden haber: entre dos símbolos del alfabeto $\mathcal{A}$, entre un símbolo y un hueco o entre un hueco y un símbolo. En definitiva:
%\[\mathbb{V}=\{\mathcal{A}\times\mathcal{A}\}\cup \{\mathcal{A}\times -\} \cup \{-\times\mathcal{A}\} \]

Ambos procesos de salida tienen el mismo espacio de posibles observaciones, y está compuesto por los elementos del alfabeto $\mathcal{A}$ y el símbolo $-$ indicando un hueco. El espacio de estados consiste principalmente en tres estados: en el estado $M$ se generan dos símbolos del alfabeto $\mathcal{A}$ mientras que en los estados $I_y$ e $I_z$ se emite un símbolo sobre una de las dos de salidas dejando un hueco en la otra. Llamaremos $p_{ab}$ a la probabilidad de que se emitan los símbolos $a$ y $b$ desde el estado $M$ y $q_{a}$ la probabilidad de emitir un símbolo $a$ y un hueco desde los estados $I_y$ e $I_z$. Usando la notación que hemos usado hasta ahora:
\[p_{ab}=P[\mathcal{Y}_k=a,\, \mathcal{Z}_k=b|\mathcal{X}_k=M]\]
\[q_a=P[\mathcal{Y}_k=a,\, \mathcal{Z}_k=-|\mathcal{X}_k=I_y]=P[\mathcal{Y}_k=-,\, \mathcal{Z}_k=a|\mathcal{X}_k=I_z]\]


A los estado $I_y$ e $I_z$ se les conocen como estados de inserción (o estado de inserción y eliminación en algunas literaturas, por ejemplo \cite{Marina}) y al estado $M$, el estado de coincidencia (\textit{match state}). 


Asumiendo que los procesos de salida son idénticos, existen dos parámetros para definir las probabilidades de transición entre estos estados: $\mu$ como probabilidad de pasar del estado $M$ a un estado de inserción ($I_y$ o $I_z$) y $\epsilon$ como la probabilidad de permanecer en un estado de inserción.

Puesto que estamos tratando con secuencias espaciales en lugar de temporales, tiene sentido definir un estado de inicio y un estado de fin. Ambos estados son estados silenciosos, estados que no producen ninguna salida. El hecho de añadir un estado de inicio facilita posteriormente en la modificación de los algoritmos. Este estado sustituye la función de la distribución inicial, de modo que el modelo siempre empezará por el estado de inicio. Para este problema, podemos considerar que el estado de inicio tenga las mismas probabilidades de transición que $M$.

Por otro lado, al añadir un estado de fin necesitamos introducir un nuevo parámetro, la probabilidad de pasar a dicho estado desde los otros estados. Asumiendo que es la misma desde cualquier de los otros estados, llamamos a dicho parámetro $\tau$. Este valor influirá en la longitud media de los alineamientos generados por el modelo. Con estos elementos, tenemos el siguiente modelo:

\begin{figure}[H]
\centering
\begin{tikzpicture}[-latex ,auto , node distance =3.5 cm ,semithick, main/.style = {draw, circle, minimum size=1.2cm}] 
    \node[main] (1) {Inicio};
    \node[main] (2) [right of=1] {$M$};
    \node[main] (3) [right of=2, above=1.4 cm of 2] {$I_y$};
    \node[main] (4) [right of=2, below=1.4 cm of 2] {$I_z$};
    \node[main] (5) [right of=3, below = 1.4cm of 3] {Fin};
    
    \path (1) edge [bend left, above, pos=0.7] node {$1-2\mu-\tau$} (2);
    \draw (1) edge [bend left=35] node {$\mu$} (3);
    \draw (1) edge [bend right=40] node {$\mu$} (4);
    \draw (1) edge [bend right=90] node {$\tau$} (5);
    
    \path (2) edge [distance=2cm, out=170, in=210, below, pos=0.7] node {$1-2\mu-\tau$} (2);
    \draw (2) edge [bend left=10] node {$\mu$} (3);
    \draw (2) edge [bend right=10, below] node {$\mu$} (4);
    \draw (2) -- node[midway, below] {$\tau$} (5);

    \path (3) edge [distance=2cm, out=330, in=30, right=1cm] node {$\epsilon$} (3);
    \draw (3) edge [bend left=10] node {$1-\epsilon-\tau$} (2);
    \draw (3) edge [bend right] node {$\tau$} (5);

    \path (4) edge [distance=2cm, out=10, in=70, right=1cm] node {$\epsilon$} (4);
    \draw (4) edge [bend right=10, right=0.1cm] node {$1-\epsilon-\tau$} (2);
    \draw (4) edge [bend right, below] node {$\tau$} (5);

    %\path (5) edge [distance=1cm, above] node {1} (5);
    
\end{tikzpicture}
\caption{\textit{Pair HMM} de Durbin}
\end{figure}
Teniendo lo anterior, podemos aplicar el algoritmo de Viterbi adaptado al modelo para encontrar el alineamiento óptimo. En este caso, la entrada consiste en dos secuencias $y=(y_1,\dots,y_n)$ y $z=(z_1,\dots,z_m)$ y la salida es la secuencia de estados que conduce al alineamiento óptimo. %Notemos que $y$ y $z$ pueden tener longitudes $n$ y $m$ diferentes y la secuencia de salida tendrá de longitud entre 

Utilizaremos los índices $i$ y $j$ para referirnos a los elementos de las secuencias $y$ y $z$. Puesto que $y_i$ y $z_j$ son elementos del alfabeto $\mathcal{A}$, por cómo hemos definido el modelo:\[p_{y_iz_j}=P[\mathcal{Y}_k=y_i,\, \mathcal{Z}_k=z_j|\mathcal{X}_k=M]\]
\[q_{y_i}=P[\mathcal{Y}_k=y_i,\, \mathcal{Z}_k=-|\mathcal{X}_k=I_y]\]
\[q_{z_j}=P[\mathcal{Y}_k=-,\, \mathcal{Z}_k=z_j|\mathcal{X}_k=I_z]\]

Recordemos que en el algoritmo original:
\[
    \delta_t(i)=\max_{(q_0,q_1\dots,q_{t-1})\in\mathbb{S}^t}P[\mathcal{X}_0^{t}=(q_0,q_1,\dots,q_{t-1},s_i),\mathcal{Y}_0^t=(O_0,\dots,O_t)]
\]

Por simplicidad, vamos a considerar $\mathbb{S}=\{M,I_y,I_z\}$ y que el estado de inicio se represente también por el estado $M$. Puesto que el estado fin es un estado silencioso y representa el final del alineamiento, tendrá un tratamiento especial. Como ahora tenemos dos índices adicionales $i$ y $j$, la variable del algoritmo de Viterbi pasa a ser de la siguiente forma dependiendo del estado que estamos considerando:
\[
\begin{aligned}
    \delta_{k}^M(i,j)= \max_{(q_0,q_1\dots,q_{k-1})\in\mathbb{S}^k}P[&\mathcal{X}_0^{k}=(q_0,q_1,\dots,q_{k-1},M),\mathcal{Y}_0^k=(u_0,\dots,u_{k-1},y_i),\\ &\mathcal{Z}_0^k=(v_0,\dots,v_{k-1},z_j)]
\end{aligned}
\] 
\[
\begin{aligned}
    \delta_{k}^{I_y}(i,j)= \max_{(q_0,q_1\dots,q_{k-1})\in\mathbb{S}^k}P[&\mathcal{X}_0^{k}=(q_0,q_1,\dots,q_{k-1},I_y),\mathcal{Y}_0^k=(u_0,\dots,u_{k-1},y_i),\\ &\mathcal{Z}_0^k=(v_0,\dots,z_j,-,\dots,-)]
\end{aligned}
\]
\[
\begin{aligned}
    \delta_{k}^{I_z}(i,j)= \max_{(q_0,q_1\dots,q_{k-1})\in\mathbb{S}^k}P[&\mathcal{X}_0^{k}=(q_0,q_1,\dots,q_{k-1},I_z),\mathcal{Y}_0^k=(u_0,\dots,y_i,-,\dots,-),\\ &\mathcal{Z}_0^k=(v_0,\dots,v_{k-1},z_j)]
\end{aligned}
\]
